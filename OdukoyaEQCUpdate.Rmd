---
title: "Odukoya Equity Center Update 6/15"
output: 
  html_document:
    css: html-md-01.css
    fig_caption: yes
    highlight: haddock
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: no
---

```{r set-options, echo=FALSE}
options(width = 105)
knitr::opts_chunk$set(dev='png', dpi=300, cache=TRUE)
pdf.options(useDingbats = TRUE)
```

# Temperature Overtime 

  I worked on familiarizing myself with the show your stripes website, climate tracking, NOAA, the berkeley earth project, and all things related to climate data that I didn't know about.  I also learned the SF package, github desktop and using github from R. Lastly, I learned about shapefiles, using ArcGIS, extracting and plotting shapefiles. 
  
### Guiding questions 
* Can we access them readily, and how (download, api, etc.)? If it can be called directly from the source into R (or Python), that’s our preferred approach.

* What’s the smallest spatial scale available? What format are they in (csv, json, excel, etc; or for spatial data, raster or vector)? What time period/years are present?

* Could we use them to measure what we think we can? What attributes/variables are available? How have they been used in related work?

* How are they generated, where do they come from (e.g., processed from instruments/satellite, derived from surveys, modeled from multiple sources, etc.)?

* key regions (FIPS): 
The larger Charlottesville area: Charlottesville City (51540), Albemarle County (51003), Fluvanna (51065), Greene (51079), Louisa (51109), Nelson (51125)
The Eastern Shore area: Accomack (51001), Northhampton (51131)

## Show your Stripes and Berkeley Earth
  
* The image is only available for state data and the raw data is not available through the site. We can download the completed graphics and use another source (https://creativecommons.org/use-remix/) to build on the graphics and reuse them if necessary. 
  
* Berkeley earth also only has state level sources. We can download a txt file for virginia as a whole but not county level data.
  
## ArcGIS

* The images are available for county data but the raw data is not available through the site. I downloaded the completed graphics for the key regions. 

  + Charlottesville City (51540)
![](images/51540-tavg.png)

  + Albemarle County (51003)
![](images/51003-tavg.png) 
  + Fluvanna (51065)
![](images/51065-tavg.png) 

  + Greene (51079)
![](images/51079-tavg.png)  

  + Louisa (51109)
![](images/51109-tavg.png)  
   
  + Nelson (51125)
![](images/51125-tavg.png)   

  + Accomack (51001)
![](images/51001-tavg.png) 

  + Northhampton (51131)
![](images/51131-tavg.png) 


## Forecast Scenarios

  The UCAR Climate Change Forecast Scenarios page (https://gisclimatechange.ucar.edu/gis-climatedata) allows downloads of data for a particular county as long as you can draw the location on the map using the box tool. It can forecast from 2006 up to year 2100 but it is only possible to download up to 10 years as shapefiles for each scenario. However, it is possible can download the full forecast as txt files. Please let me know if you would like me to download the data for the key regions and for what date ranges and scenarios. For shapefiles, I can download a 100 year forecast in 10 downloads if that is preferred to txt files.
  
  I found two other forecast tools but they are not as good as the UCAR website. 
  
  + The CREAT CLimate Scenarios Projection Map (https://www.arcgis.com/apps/MapSeries/index.html?appid=3805293158d54846a29f750d63c6890e) can show forecasts for year 2035 and 2060 for temperature, percipitation, storms, extreme heat, and sea levels. It is possible to get county forecast by zooming into the map and clicking on the county of interest. However, it is not possible to download the data.
  
  + The NOAA CM2.X dataset contains the results of two models run by the NOAA’s Geophysical Fluid Dynamics Laboratory (GFDL). They show general climate conditions during the 20th century and projections into the 21st century based on various climate scenarios. It can be downloaded at (https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/cm2-global-coupled-climate-models-cm2x). While it does not show the map of the forecasts, we can download the data and create our own. This is also a global forecast so chances of extracting county level data might be slim. It covers January 1, 1901 to April 21, 2300. 
  
## NOAA Data

  NOAA has data that covers county, regional, state, country, and global levels. NOAA data is extractable in txt format and through API for date ranges 1895-2021.
  
  + Monthly readings for each county measuring 
    + 01 = Precipitation
    + 02 = Average Temperature
    + 27 = Maximum Temperature
    + 28 = Minimum Temperature

  + The data is used to calculate the state temperature which is then used in regional and country-wide calculations. Minimum and Maximum would be useful if we wanted to run our own calculations but the average temperature is usually a standard measurement of temperature overtime.

  The nClimDiv dataset contains the county information that we are looking to access from NOAA. The dataset is derived from the station information in the daily Global Historical Climatology Network (GHCN) dataset. Therefore, the nClimDiv is the processed data format of the GHCN and would be more useful to us than getting raw GHCN data. 
  
  My NOAA script so far is: 
  
```{r libs, message=FALSE, warning=FALSE}
library(knitr)
library(dplyr)
library(tidyverse)
library(plotly)
library(gridExtra)
library(vars)
library(urca)
library(MASS)
library(stargazer)
library(sf)
library(stringi)
library(leaflet)
library(raster)
library(tmap)
library(mapdeck)
library(mapview)
library(RColorBrewer)
library(ggplot2)
library(gganimate)
library(gapminder)
library(transformr)
library(gifski)
library(ggrepel)
library(hrbrthemes)
```

  This was to load the average temperature data 
```{r load new data}
climcounty <- read.table(file = "climdiv-tmpccy-v1.0.0-20210604.txt", header = FALSE)
head(climcounty)
```

This was to load the county data with shapefiles
```{r load new data shp}
ccshape <- st_read("/Users/msbugatti/Documents/Intership 2021/NOAA Temp Data/counties")
```

This was to filter the county data to the counties of interest
```{r filter}
ccshape1 <- ccshape %>% filter(FIPS %in% c(51540, 51003, 51065, 51079, 51109, 51125, 51001, 51131))
```

This was my first run at ploting the data. It is very bland.
```{r ggplot}
ggplot() + 
  geom_sf(data = ccshape1, size = 3, color = "black", fill = "orange") + 
  ggtitle("Virginia Counties Plot") + 
  coord_sf()
```


### Goals for this week 

* Get NOAA data, clean it, and explore the observations (distributions across spatial units, summaries by spatial units, etc.) 

* Learn lovelace and other ploting skills for shapefiles in R

* Practice GitHub from terminal instead of GitHub desktop which is much more easy for me to use.
  
* Lastly, a gif of my life this week 

  ![](https://media.giphy.com/media/3o7TKJ7stEm0x8LYOc/giphy.gif)
  
# Week Two and Three

After spending most of week two figuring out the data mapping. I worked on the animated map this week and spent more time trying to figure out why my map was displaying two labels for one frame during the animation process. I finally got it to work last night but am now dealing with a .nc read problem with the DAYMET dataset. This should be faster once I can read it into R. I am excited to work on the strips and yearly average data for the NOAA dataset. 

## My code and the animated map for july are below.

```{r}
climcounty <- rename(climcounty, Fipsyear=V1, Jan=V2, Feb=V3, Mar=V4, Apr=V5, May=V6, Jun=V7, Jul=V8, Aug=V9, Sep=V10, Oct=V11, Nov=V12, Dec=V13)
```

```{r filter counties}
countyavgtemp <- climcounty %>% filter(Fipsyear %in% c(44540021895:44540022021, 44003021895:44003022021, 44065021895:44065022021, 44079021895:44079022021, 44109021895:44109022021, 44125021895:44125022021, 44001021895:44001022021, 44131021895:44131022021))
```


```{r}
countyavgtemp1 <- countyavgtemp %>%
  group_by(Fipsyear) %>% mutate(year = as.numeric(str_sub(Fipsyear, 8, )), CNTY_FIPS = str_sub(Fipsyear, 3, -7))
```

```{r}
countyavgtemp1 <- countyavgtemp1 %>%
                      dplyr::select(CNTY_FIPS, year, Jan, Feb,
                                Mar, Apr, May, Jun, Jul, Aug, Sep,
                                Oct, Nov, Dec, Fipsyear)
```

```{r}
ccshapegf <- left_join(ccshape1, countyavgtemp1, by ="CNTY_FIPS")
```


```{r}
countyavgsf <- st_as_sf(ccshapegf)
```

```{r}
ccshapegf <- cbind(ccshapegf, st_coordinates(st_centroid(ccshapegf)))
```


```{r}
ccshapeggf <- 
  ggplot(ccshapegf) +
  geom_sf(aes(fill = Jul), color = "black", alpha = .9) +
   geom_text_repel(data = ccshapegf, aes(X, Y, label = NAME), size = 4, nudge_x = 1, nudge_y = 0, fontface = "bold", hjust = 0.9) +
  scale_fill_fermenter(palette = "YlOrRd", direction = 1,   type = "seq", n.breaks = 8) +
     theme_void() +
  guides(fill =
           guide_colourbar(title.position="top", title.hjust = 0.5,
                           barwidth = 1)
  )  + 
  labs(fill = "Temperature ", title = 'Year: {frame_time}',
       caption = "Average temperature in July for 7 Counties") + 
 transition_time(as.integer(year)) +
ease_aes('linear') 
```

```{r}
animate(ccshapeggf, fps = 1, detail = 1, nframes = 127)
```


### Goals for this Week 

* Finish the DAYMET data clean.

* Work on the creating the stripes and a yearly average temperature data to create a new animation.

* Create the rmd scripts, csv output data, and data review file for NOAA and DAYMET. 

* Work on the new goals for this week. 